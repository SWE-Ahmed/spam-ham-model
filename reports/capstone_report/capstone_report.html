<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.640">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ahmed Almohammed">
<meta name="dcterms.date" content="2022-08-25">

<title>Capstone Report on Spam Detection Model Implemented on a Telegram Bot</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="capstone_report_files/libs/clipboard/clipboard.min.js"></script>
<script src="capstone_report_files/libs/quarto-html/quarto.js"></script>
<script src="capstone_report_files/libs/quarto-html/popper.min.js"></script>
<script src="capstone_report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="capstone_report_files/libs/quarto-html/anchor.min.js"></script>
<link href="capstone_report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="capstone_report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="capstone_report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="capstone_report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="capstone_report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data Preprocessing</a>
  <ul class="collapse">
  <li><a href="#oversampling-the-minority-class" id="toc-oversampling-the-minority-class" class="nav-link" data-scroll-target="#oversampling-the-minority-class">Oversampling the Minority Class</a></li>
  <li><a href="#undersampling-the-majority-class" id="toc-undersampling-the-majority-class" class="nav-link" data-scroll-target="#undersampling-the-majority-class">Undersampling the Majority Class</a></li>
  <li><a href="#text-tokenization" id="toc-text-tokenization" class="nav-link" data-scroll-target="#text-tokenization">Text Tokenization</a></li>
  </ul></li>
  <li><a href="#model-development" id="toc-model-development" class="nav-link" data-scroll-target="#model-development">Model Development</a>
  <ul class="collapse">
  <li><a href="#model-0-baseline-model" id="toc-model-0-baseline-model" class="nav-link" data-scroll-target="#model-0-baseline-model">Model 0: Baseline Model</a></li>
  <li><a href="#model-1-simple-deep-model" id="toc-model-1-simple-deep-model" class="nav-link" data-scroll-target="#model-1-simple-deep-model">Model 1: Simple Deep Model</a></li>
  <li><a href="#model-2-deep-model-with-lstm" id="toc-model-2-deep-model-with-lstm" class="nav-link" data-scroll-target="#model-2-deep-model-with-lstm">Model 2: Deep Model with LSTM</a></li>
  <li><a href="#model-3-deep-model-with-gru" id="toc-model-3-deep-model-with-gru" class="nav-link" data-scroll-target="#model-3-deep-model-with-gru">Model 3: Deep Model with GRU</a></li>
  <li><a href="#model-4-deep-model-with-brnn" id="toc-model-4-deep-model-with-brnn" class="nav-link" data-scroll-target="#model-4-deep-model-with-brnn">Model 4: Deep Model with BRNN</a></li>
  <li><a href="#model-5-deep-model-with-conv1d" id="toc-model-5-deep-model-with-conv1d" class="nav-link" data-scroll-target="#model-5-deep-model-with-conv1d">Model 5: Deep Model with Conv1D</a></li>
  <li><a href="#model-6-deep-model-with-modifications" id="toc-model-6-deep-model-with-modifications" class="nav-link" data-scroll-target="#model-6-deep-model-with-modifications">Model 6: Deep Model with Modifications</a></li>
  <li><a href="#model-conclusion" id="toc-model-conclusion" class="nav-link" data-scroll-target="#model-conclusion">Model Conclusion</a></li>
  </ul></li>
  <li><a href="#model-deployment" id="toc-model-deployment" class="nav-link" data-scroll-target="#model-deployment">Model Deployment</a></li>
  <li><a href="#problems-associated-with-current-solution" id="toc-problems-associated-with-current-solution" class="nav-link" data-scroll-target="#problems-associated-with-current-solution">Problems Associated with Current Solution</a></li>
  <li><a href="#future-development" id="toc-future-development" class="nav-link" data-scroll-target="#future-development">Future Development</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Capstone Report on Spam Detection Model Implemented on a Telegram Bot</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ahmed Almohammed </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 25, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<p>This report illustrates the fundamental phases of the project, from data preprocessing to deployment using <code>pyrogram</code> library to build a working Telegram Bot capable of using the ML model generated to monitor the messages within a groupchat, and flag any messages that are considered potential spam.</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>This capstone project addresses the problem of spam messages being sent out via various users to public groupchats. A possible solution has been built in this project, in which a deep learning model was trained on sms messages, labelled as either spam or not, and then used in a Telegram bot to detect incoming spam messages in the deployed Telegram groupchats.</p>
</section>
<section id="data-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="data-preprocessing">Data Preprocessing</h2>
<p>The data for this project was gathered from 4 various sources:</p>
<ul>
<li><a href="https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset">uciml</a> from Kaggle</li>
<li><a href="https://www.kaggle.com/datasets/team-ai/spam-text-message-classification">team-ai</a> from Kaggle</li>
<li><a href="https://github.com/DeshDSingh/SMS-SPAM-Detection/blob/master/sms_spam.csv">DeshDSingh</a> from GitHub</li>
<li><a href="https://huggingface.co/datasets/sms_spam">sms_spam</a> from HuggingFace Datasets</li>
</ul>
<p>After getting the data from their sources, they were all concatenated together, dropping the resulting duplicates, to finally be left out with a large volume of data, totalling at around <code>11K</code> examples, with the following data description:</p>
<table class="table">
<colgroup>
<col style="width: 45%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">message</td>
<td style="text-align: center;">the sms message in text form</td>
</tr>
<tr class="even">
<td style="text-align: left;">label</td>
<td style="text-align: center;">the classification of the message, either <code>spam</code>, or not, <code>ham</code></td>
</tr>
</tbody>
</table>
<p>However, the data was heavily imbalanced, especially in the original sources, where the data with label <code>ham</code> were by far the dominating example in the dataset. See <a href="#fig-class">Figure&nbsp;1</a>.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the distribution of the examples in each class</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>,<span class="dv">6</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="st">'label'</span>, data<span class="op">=</span>sms_big)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Distribution of the Messages'</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Labels'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Count'</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-class" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="capstone_report_files/figure-html/fig-class-output-1.png" width="759" height="515" class="figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 1: The imbalance between the 2 classes</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To solve this imbalance problem, 2 methods were tried (after performing train-test split):</p>
<section id="oversampling-the-minority-class" class="level3">
<h3 class="anchored" data-anchor-id="oversampling-the-minority-class">Oversampling the Minority Class</h3>
<p>The idea was to use <code>SMOTEN</code> method from <code>imblearn</code> package to successfully oversample the <code>ham</code> class. Upon creating the <code>SMOTEN</code> instance, and applying it to our data sets, the classes became balanced. However, the minority class was almost three times filled with duplicates, which is extremely unwanted in our data, especially when we proceed to model development, as the model will no doubt overfit the data and will not generalize well on unseen data.</p>
<p>As a result, the next method was tried.</p>
</section>
<section id="undersampling-the-majority-class" class="level3">
<h3 class="anchored" data-anchor-id="undersampling-the-majority-class">Undersampling the Majority Class</h3>
<p>In performing undersampling for the majority class, we used the <code>RandomUnderSampler</code> method from <code>imblearn</code>, and then applied it on the data sets. Even though many examples from the <code>ham</code> class were truncated and left off to perform undersampling, this is still better in this case than having a <code>spam</code> class consisting of almost all duplicates.</p>
<p>As a result, the total number of examples left went down from about <code>11K</code> to about <code>3K</code>. <a href="#fig-train">Figure&nbsp;2</a> and <a href="#fig-test">Figure&nbsp;3</a> depict the distribution of the data in train and test sets respectively.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># training data after undersampling</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X_res, y_res <span class="op">=</span> unders.fit_resample(np.array(X_train).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), np.array(y_train).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X_train_res <span class="op">=</span> pd.Series(X_res.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y_train_res <span class="op">=</span> pd.Series(y_res.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.concat([X_train_res, y_train_res], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="dv">1</span>, data<span class="op">=</span>train_df)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-train" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="capstone_report_files/figure-html/fig-train-output-1.png" width="601" height="422" class="figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 2: The training data set</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># testing data after undersampling</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>X_res, y_res <span class="op">=</span> unders.fit_resample(np.array(X_test).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), np.array(y_test).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X_test_res <span class="op">=</span> pd.Series(X_res.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>y_test_res <span class="op">=</span> pd.Series(y_res.reshape(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.concat([X_test_res, y_test_res], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="dv">1</span>, data<span class="op">=</span>test_df)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-test" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="capstone_report_files/figure-html/fig-test-output-1.png" width="593" height="422" class="figure-img"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 3: The testing data set</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>After splitting the data, and fixing the imbalance problem in the right way, we can proceed to the next phase of the preprocessing, whcih is Text Tokenization.</p>
</section>
<section id="text-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="text-tokenization">Text Tokenization</h3>
<p>Now, we must convert our categorical data, i.e the <code>message</code> and the <code>label</code>, into numerical data, as the deep learning models cannot handle text data, it needs to be converted to numbers. For the <code>label</code>, we can easily convert them to numerical data through using <code>LabelEncoder</code> frok <code>sklearn</code> package. After doing so, the result of applying the <code>LabelEncoder</code> looks like this:</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Before applying LabelEncoder: </span><span class="sc">{</span>y_train_res[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># apply label encoder to transform the caategorical target variable</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>le <span class="op">=</span> LabelEncoder()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>y_train_res <span class="op">=</span> le.fit_transform(y_train_res)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>y_test_res <span class="op">=</span> le.fit_transform(y_test_res)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"After applying LabelEncoder: </span><span class="sc">{</span>y_train_res[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Before applying LabelEncoder: ['ham' 'ham' 'ham' 'ham' 'ham']
After applying LabelEncoder: [0 0 0 0 0]</code></pre>
</div>
</div>
<p>Next, we will need to first apply a Text Vectorizer on our <code>message</code> feature, and then initialize an embedding layer to be used during our experimenting with model building. The Text Vectorizer applied in this case assigns a unique number to each word (word-level tokenization) in the <code>message</code> feature, i.e the text corpus, where each word in this case is considered a token. Both the text vectorizer, along with the embedding, are used from the <code>tensorflow</code> deep learning package. After applying the text vectorization on the data, and initiating an embedded layer to be used later on, this is the result attained:</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original message:</span><span class="ch">\n</span><span class="sc">{</span>X_train_res[<span class="dv">0</span>]<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">Vectorized version:</span><span class="ch">\n</span><span class="sc">{</span>text_vectorizer([X_train_res[<span class="dv">0</span>]])<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">Embedded version:</span><span class="ch">\n</span><span class="sc">{</span>embedding(text_vectorizer([X_train_res[<span class="dv">0</span>]]))<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original message:
Siva is in hostel aha:-.


Vectorized version:
[[4008   11   17 1973 3294    0    0    0    0    0    0    0    0    0
     0    0    0    0    0]]

Embedded version:
[[[-0.04009664 -0.0182348   0.04175608 ...  0.01563163 -0.04268491
   -0.00099881]
  [-0.02363955  0.01625723 -0.03082242 ... -0.00070429 -0.0444646
    0.00976185]
  [-0.04665051 -0.00272837 -0.01892921 ... -0.00298215 -0.012444
    0.0101974 ]
  ...
  [ 0.01520063  0.00729283 -0.01572526 ... -0.01472719  0.0357582
   -0.0234138 ]
  [ 0.01520063  0.00729283 -0.01572526 ... -0.01472719  0.0357582
   -0.0234138 ]
  [ 0.01520063  0.00729283 -0.01572526 ... -0.01472719  0.0357582
   -0.0234138 ]]]</code></pre>
</div>
</div>
<p>Finally, we can now proceed to model development, as we have completed all the preprocessing necessary on our data.</p>
</section>
</section>
<section id="model-development" class="level2">
<h2 class="anchored" data-anchor-id="model-development">Model Development</h2>
<p>In this phase, several deep learning model architectures were tested out on the training and test sets, including a baseline model built using a shallow learning algorithm. Because of the low number of training examples due to the heavily imbalanced problem and the undersampling made, having a high accuracy did not necessarily indicate that the model will perform well on custom data, i.e it wont necessary generalize well to unseen messages. As a result, I have created a second metric consisting of sample custom data to also use in finding the optimal model in this scenario.</p>
<p>Below are the architectures used for each model, and a brief overview of why it was tested out:</p>
<section id="model-0-baseline-model" class="level3">
<h3 class="anchored" data-anchor-id="model-0-baseline-model">Model 0: Baseline Model</h3>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>model_0 <span class="op">=</span> Pipeline([</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'tfid'</span>, TfidfVectorizer()),</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'mnb'</span>, MultinomialNB())</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>model_0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[('tfid', TfidfVectorizer()), ('mnb', MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox"><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('tfid', TfidfVectorizer()), ('mnb', MultinomialNB())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">TfidfVectorizer</label><div class="sk-toggleable__content"><pre>TfidfVectorizer()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">MultinomialNB</label><div class="sk-toggleable__content"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>
</div>
</div>
<p>This model acts as the starting ground for model development, as I try to better develop a deep neural network architecture to beat it. It uses <code>TfidVectorizer</code> (term frequency-inverse document frequency) method to map the words in our messages to unique numbers. Then it passes the output to <code>MultinomialNB</code>, which is scikit-learn go to for classification problems regarding text data.</p>
</section>
<section id="model-1-simple-deep-model" class="level3">
<h3 class="anchored" data-anchor-id="model-1-simple-deep-model">Model 1: Simple Deep Model</h3>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build the model</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">'string'</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> text_vectorizer(inputs)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> embedding(x)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalAvgPool1D()(x)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>model_1 <span class="op">=</span> Model(inputs, outputs, name<span class="op">=</span><span class="st">'simple_model_1'</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>model_1.<span class="bu">compile</span>(</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span> Adam(),</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># get a summary of the model</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>model_1.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "simple_model_1"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> input_1 (InputLayer)        [(None, 1)]               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> text_vectorization (TextVec  (None, 19)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> torization)                                                     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> embedding (Embedding)       (None, 19, 128)           720896    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> global_average_pooling1d (G  (None, 128)              0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> lobalAveragePooling1D)                                          </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense (Dense)               (None, 1)                 129       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 721,025</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 721,025</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>This model starts off easy with only the usual text vectorization and embedding layers, and then a <code>GlobalAvgPool1D</code> layer to reduce the size of the representation and detect more robust features.</p>
</section>
<section id="model-2-deep-model-with-lstm" class="level3">
<h3 class="anchored" data-anchor-id="model-2-deep-model-with-lstm">Model 2: Deep Model with LSTM</h3>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model_2 embedding layer</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> Embedding(input_dim<span class="op">=</span><span class="bu">len</span>(text_vectorizer.get_vocabulary()), </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>                             output_dim<span class="op">=</span><span class="dv">128</span>, </span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>                             embeddings_initializer<span class="op">=</span><span class="st">"uniform"</span>, </span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>                             input_length<span class="op">=</span><span class="dv">19</span>) </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># build the model</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">'string'</span>)</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> text_vectorizer(inputs)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> embedding(x)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> LSTM(<span class="dv">128</span>)(x)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>model_2 <span class="op">=</span> Model(inputs, outputs, name<span class="op">=</span><span class="st">'lstm_model_2'</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>model_2.<span class="bu">compile</span>(</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>Adam(),</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="co"># summary of the model</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>model_2.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "lstm_model_2"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> input_2 (InputLayer)        [(None, 1)]               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> text_vectorization (TextVec  (None, 19)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> torization)                                                     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> embedding_1 (Embedding)     (None, 19, 128)           720896    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> lstm (LSTM)                 (None, 128)               131584    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_1 (Dense)             (None, 1)                 129       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 852,609</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 852,609</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>We must redefine a new embedding layer to each model, as the embedding layer is considered a learned representation, where each model might have different representation for it based on its training. In this model, we used the first type of RNN, and LSTM layer (Long-Shot-Term-Memory). Its a sophisticated RNN that uses 3 main components in its computations, namely an input gate, update gate and forget gate. Together, these tend to produce accurate results for long sequences (as in our case)</p>
</section>
<section id="model-3-deep-model-with-gru" class="level3">
<h3 class="anchored" data-anchor-id="model-3-deep-model-with-gru">Model 3: Deep Model with GRU</h3>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model_3 embedding layer</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> Embedding(input_dim<span class="op">=</span><span class="bu">len</span>(text_vectorizer.get_vocabulary()), </span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>                             output_dim<span class="op">=</span><span class="dv">128</span>, </span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>                             embeddings_initializer<span class="op">=</span><span class="st">"uniform"</span>, </span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>                             input_length<span class="op">=</span><span class="dv">19</span>) </span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="co"># build the model</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">'string'</span>)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> text_vectorizer(inputs)</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> embedding(x)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GRU(<span class="dv">128</span>)(x)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a>model_3 <span class="op">=</span> Model(inputs, outputs, name<span class="op">=</span><span class="st">'gru_model_3'</span>)</span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a>model_3.<span class="bu">compile</span>(</span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>Adam(),</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a><span class="co"># summary of model_3</span></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a>model_3.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "gru_model_3"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> input_3 (InputLayer)        [(None, 1)]               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> text_vectorization (TextVec  (None, 19)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> torization)                                                     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> embedding_2 (Embedding)     (None, 19, 128)           720896    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> gru (GRU)                   (None, 128)               99072     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_2 (Dense)             (None, 1)                 129       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 820,097</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 820,097</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>We also tried the second type of an RNN, namely GRU (Gated Recurrent Network). This type is not as computationaly costly as the LSTM, as it tends to have lower parameters due to its less component architecture under the hood. Even though it uses less memory and is faster than LSTM, it might not perform so well in this case.</p>
</section>
<section id="model-4-deep-model-with-brnn" class="level3">
<h3 class="anchored" data-anchor-id="model-4-deep-model-with-brnn">Model 4: Deep Model with BRNN</h3>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model_4 embedding layer</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> Embedding(input_dim<span class="op">=</span><span class="bu">len</span>(text_vectorizer.get_vocabulary()), </span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>                             output_dim<span class="op">=</span><span class="dv">128</span>, </span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>                             embeddings_initializer<span class="op">=</span><span class="st">"uniform"</span>, </span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>                             input_length<span class="op">=</span><span class="dv">19</span>) </span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="co"># build the model</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">'string'</span>)</span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> text_vectorizer(inputs)</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> embedding(x)</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Bidirectional(LSTM(<span class="dv">128</span>))(x)</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>model_4 <span class="op">=</span> Model(inputs, outputs, name<span class="op">=</span><span class="st">'brnn_model_4'</span>)</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a>model_4.<span class="bu">compile</span>(</span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>Adam(),</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a><span class="co"># summary of model</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a>model_4.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "brnn_model_4"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> input_4 (InputLayer)        [(None, 1)]               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> text_vectorization (TextVec  (None, 19)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> torization)                                                     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> embedding_3 (Embedding)     (None, 19, 128)           720896    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> bidirectional (Bidirectiona  (None, 256)              263168    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> l)                                                              </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_3 (Dense)             (None, 1)                 257       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 984,321</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 984,321</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>Instead of only making the training process unidirectional, we tested out on making it bidirectional, to make the alogirthm or the the network detect more features out of the data. Its use was especially intended as it can learn from the context of the message, as we think it might affect the classification of the message.</p>
</section>
<section id="model-5-deep-model-with-conv1d" class="level3">
<h3 class="anchored" data-anchor-id="model-5-deep-model-with-conv1d">Model 5: Deep Model with Conv1D</h3>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model_5 embedding layer</span></span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> Embedding(input_dim<span class="op">=</span><span class="bu">len</span>(text_vectorizer.get_vocabulary()), </span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>                             output_dim<span class="op">=</span><span class="dv">128</span>, </span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>                             embeddings_initializer<span class="op">=</span><span class="st">"uniform"</span>, </span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a>                             input_length<span class="op">=</span><span class="dv">19</span>) </span>
<span id="cb95-6"><a href="#cb95-6" aria-hidden="true" tabindex="-1"></a><span class="co"># build the model</span></span>
<span id="cb95-7"><a href="#cb95-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">'string'</span>)</span>
<span id="cb95-8"><a href="#cb95-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> text_vectorizer(inputs)</span>
<span id="cb95-9"><a href="#cb95-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> embedding(x)</span>
<span id="cb95-10"><a href="#cb95-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Conv1D(filters<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb95-11"><a href="#cb95-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalMaxPool1D()(x)</span>
<span id="cb95-12"><a href="#cb95-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb95-13"><a href="#cb95-13" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb95-14"><a href="#cb95-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-15"><a href="#cb95-15" aria-hidden="true" tabindex="-1"></a>model_5 <span class="op">=</span> Model(inputs, outputs, name<span class="op">=</span><span class="st">'conv1d_model_5'</span>)</span>
<span id="cb95-16"><a href="#cb95-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb95-17"><a href="#cb95-17" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb95-18"><a href="#cb95-18" aria-hidden="true" tabindex="-1"></a>model_5.<span class="bu">compile</span>(</span>
<span id="cb95-19"><a href="#cb95-19" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb95-20"><a href="#cb95-20" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>Adam(),</span>
<span id="cb95-21"><a href="#cb95-21" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb95-22"><a href="#cb95-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb95-23"><a href="#cb95-23" aria-hidden="true" tabindex="-1"></a><span class="co"># summary of model</span></span>
<span id="cb95-24"><a href="#cb95-24" aria-hidden="true" tabindex="-1"></a>model_5.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "conv1d_model_5"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> input_5 (InputLayer)        [(None, 1)]               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> text_vectorization (TextVec  (None, 19)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> torization)                                                     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> embedding_4 (Embedding)     (None, 19, 128)           720896    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> conv1d (Conv1D)             (None, 15, 32)            20512     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> global_max_pooling1d (Globa  (None, 32)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> lMaxPooling1D)                                                  </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_4 (Dense)             (None, 32)                1056      </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_5 (Dense)             (None, 1)                 33        </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 742,497</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 742,497</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>The use of <code>Conv1D</code> layer is of a great benefit to the training of the model, as this layer allows our network to capture the spatial data from our 1 dimensional sequences of messages, in which other RNN layers were unable to do so. Through this layer, we can utilize the power of convolutional layers used in effective computer vision problems.</p>
</section>
<section id="model-6-deep-model-with-modifications" class="level3">
<h3 class="anchored" data-anchor-id="model-6-deep-model-with-modifications">Model 6: Deep Model with Modifications</h3>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create model_6 embedding layer</span></span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> Embedding(input_dim<span class="op">=</span><span class="bu">len</span>(text_vectorizer.get_vocabulary()), </span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>                             output_dim<span class="op">=</span><span class="dv">128</span>, </span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a>                             embeddings_initializer<span class="op">=</span><span class="st">"uniform"</span>, </span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true" tabindex="-1"></a>                             input_length<span class="op">=</span><span class="dv">19</span>) </span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true" tabindex="-1"></a><span class="co"># build the model</span></span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span><span class="st">'string'</span>)</span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> text_vectorizer(inputs)</span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> embedding(x)</span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Bidirectional(LSTM(<span class="dv">256</span>))(x)</span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Reshape((<span class="dv">512</span>,<span class="dv">1</span>))(x)</span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Conv1D(filters<span class="op">=</span><span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GlobalMaxPool1D()(x)</span>
<span id="cb121-14"><a href="#cb121-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb121-15"><a href="#cb121-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb121-16"><a href="#cb121-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">16</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb121-17"><a href="#cb121-17" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> Dense(units<span class="op">=</span><span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb121-18"><a href="#cb121-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-19"><a href="#cb121-19" aria-hidden="true" tabindex="-1"></a>model_6 <span class="op">=</span> Model(inputs, outputs, name<span class="op">=</span><span class="st">'mods_model_6'</span>)</span>
<span id="cb121-20"><a href="#cb121-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb121-21"><a href="#cb121-21" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb121-22"><a href="#cb121-22" aria-hidden="true" tabindex="-1"></a>model_6.<span class="bu">compile</span>(</span>
<span id="cb121-23"><a href="#cb121-23" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb121-24"><a href="#cb121-24" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>Adam(),</span>
<span id="cb121-25"><a href="#cb121-25" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb121-26"><a href="#cb121-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb121-27"><a href="#cb121-27" aria-hidden="true" tabindex="-1"></a><span class="co"># summary of model</span></span>
<span id="cb121-28"><a href="#cb121-28" aria-hidden="true" tabindex="-1"></a>model_6.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "mods_model_6"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> input_6 (InputLayer)        [(None, 1)]               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> text_vectorization (TextVec  (None, 19)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> torization)                                                     </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> embedding_5 (Embedding)     (None, 19, 128)           720896    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> bidirectional_1 (Bidirectio  (None, 512)              788480    </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> nal)                                                            </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> reshape (Reshape)           (None, 512, 1)            0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> conv1d_1 (Conv1D)           (None, 508, 32)           192       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> global_max_pooling1d_1 (Glo  (None, 32)               0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> balMaxPooling1D)                                                </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_6 (Dense)             (None, 32)                1056      </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dropout (Dropout)           (None, 32)                0         </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_7 (Dense)             (None, 16)                528       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_8 (Dense)             (None, 1)                 17        </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 1,511,169</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 1,511,169</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>In this final model, we implemented a modified deep neural network architecture based on our experimenting with the above models and their components. After the ususal text vectorization and embedding layers, the output gets passed to a bidirectional <code>LSTM</code> layer to accurately detect features from the sequence of messages in both directions and based on the context of them in general. It then gets reshaped to fit into a <code>Conv1D</code> layer to further detect more features on top of the <code>LSTM</code> ones, through the power of convolotions. It then outputs it to a combination of max pooling and fully connected layers to reduce representation size and also a <code>Dropout</code> layer for regularization.</p>
</section>
<section id="model-conclusion" class="level3">
<h3 class="anchored" data-anchor-id="model-conclusion">Model Conclusion</h3>
<p>Based on the above models, each model was trained on the same metrics and hyperparameters to ensure fairness for all. But like we mentioned above, becuase of the low data, the accuracy and f1-score metrics were not really helpful. Several models tend to attains almost similar results, while others unfortunately overfitted the data. With the custom data used as an additional metric, the results were as follows:</p>

<table class="table table-hover">
<thead>
<tr>
<th scope="col">
Model
</th>
<th scope="col">
Accuracy
</th>
<th scope="col">
F1-Score
</th>
<th scope="col">
Custom Data
</th>
</tr>
</thead>
<tbody>

<tr class="table-info">
<th scope="row">
Model 0
</th>
<td>
0.9599
</td>
<td>
0.9599
</td>
<td>
0.7500
</td>
</tr>

<tr class="table-info">
<th scope="row">
Model 1
</th>
<td>
0.9599
</td>
<td>
0.9598
</td>
<td>
0.5000
</td>
</tr>

<tr class="table-info">
<th scope="row">
Model 2
</th>
<td>
0.9775
</td>
<td>
0.9775
</td>
<td>
0.7500
</td>
</tr>

<tr class="table-info">
<th scope="row">
Model 3
</th>
<td>
0.9855
</td>
<td>
0.9855
</td>
<td>
0.7500
</td>
</tr>

<tr class="table-info">
<th scope="row">
Model 4
</th>
<td>
0.9727
</td>
<td>
0.9727
</td>
<td>
0.7500
</td>
</tr>

<tr class="table-info">
<th scope="row">
Model 5
</th>
<td>
0.9839
</td>
<td>
0.9839
</td>
<td>
0.5000
</td>
</tr>

<tr class="table-success">
<th scope="row">
Model 6
</th>
<td>
0.9743
</td>
<td>
0.9743
</td>
<td>
1.000
</td>
</tr>
</tbody>

</table>
<p>Based on the above results, we concluded that <code>model_6</code> is the optimal model for this case, as it can generalize well to unseen data, both to the custom data here and other ones used in the raw notebook.</p>
<p>Now, the model is ready to be deployed.</p>
</section>
</section>
<section id="model-deployment" class="level2">
<h2 class="anchored" data-anchor-id="model-deployment">Model Deployment</h2>
<p>After the model development and experimenting with the various layers, we finally have a good model to use in our intended case, and that is deploy it to a Telegram groupchat and see it working in action. The following points illustrate the main steps taken into deploying a fully working Telegram bot capable of utilizing our model’s predictions in detecting spam:</p>
<ol type="1">
<li>Save the model</li>
</ol>
<p>The model was saved through the use of <code>TensorFlow</code>’s <code>save_model</code> method, which saves all the needed data for the model to work successfully when loaded in a separate folder.</p>
<ol start="2" type="1">
<li>Initializing a Telegram Bot with the help of <code>Pyrogram</code></li>
</ol>
<p>For this part, the Telegram bot was created with the help of several functions from the <code>pyrogram</code> package, which included</p>
<ul>
<li>Getting the api configuration keys</li>
<li>Creating a bot account from the <code>BotFather</code> in Telegram</li>
<li>Setting a user session along with a Bot session</li>
<li>Adding handlers for required functionalities for the bot</li>
</ul>
<ol start="3" type="1">
<li>Load the model and register it to a handler</li>
</ol>
<p>The saved model can be easily loaded up and get it ready for prediction through <code>TensorFlow</code>’s <code>load_model</code> method. After loading the model with its weights and data, a handler function is created to monitor the messages sent in the groupchat in the following way:</p>
<ul>
<li><p>If the message is detected as potential spam, then the user gets a warning and the message is flagged as potential spam. Once reaching a 3rd warning, the last potential spam message will be deleted, and the user will be banned.</p></li>
<li><p>If the message is detected as <code>ham</code> (not spam), then the bot just ignores it.</p></li>
</ul>
<p>For more info on the deployment implementation, see the <code>app</code> directory.</p>
</section>
<section id="problems-associated-with-current-solution" class="level2">
<h2 class="anchored" data-anchor-id="problems-associated-with-current-solution">Problems Associated with Current Solution</h2>
<p>As we can see from the performance of our model in classifying incoming messages in these Telegram chats, we notice that sometimes there are case were the model misclassifies a non-spam message for a spam message. This is due to several factors that have limited our model’s performance in such way, mainly include:</p>
<ul>
<li><strong>Unrepresentable Data</strong>: This is very probelmatic for our implementation, as the data that the model was trained on turned out to be not representable of the whole diversity and strucutre of the messages in sent by users in Telegram.</li>
<li><strong>Not Enough Data</strong>: As we mentioned in data preprocessing phase, because we had this problem of class imbalance and we had the need to undersample our data to level it with out minority class, many examples were left out and were left with only very few training data compared to how we started with. As a result, our model could not successfully learn the distinguishable points between spam and not spam messages.</li>
<li><strong>Nature of Spam Messages</strong>: As we can see in our modern day social media activity, we can all agree that the spam messages circulated between users and groupchats are not of consistent structure, and they tend to learn and improve their delivery of spam messages to further improve their chance of manipulating the user. It is quite hard to get hands on such data from various sources to ensure variability in the spam messages, and even harder in making our model keep up with the new types of spam messages.</li>
</ul>
</section>
<section id="future-development" class="level2">
<h2 class="anchored" data-anchor-id="future-development">Future Development</h2>
<p>Despite the ups and downs of this project, there have been many new ideas and thoughts in mind throughout the development phase (raises some ethical concerns):</p>
<ul>
<li><strong>Scraping Messages for new Training Data in Telegram Groupchats</strong>: One of the main problems in this project was that the data acquired did not really match the structure of the messages sent in Telegram chats. As a result, to fix this for future needs, having a data purely from Telegram groupchats will definitely improve the model performance. However, before scraping the messages from any groupchats, the members should be notified of such action, and their permission must be given to commence such operation.</li>
<li><strong>Storing Successfully Predicted Messages in a Data Warehouse</strong>: Instead of having to go through the ethical concerns with scraping data from Telegram groupchats, we can make the deployed Bot in a groupchat store the successful predicted spam messages in some place like a data warehouse (csv file, on the cloud), where we can later use this set of data to better optimize our model’s performance.</li>
<li><strong>Improving the Handling of Spam by the Bot</strong>: The current handling is done through giving warnings to potential spam senders, and banning them once a 3rd warning is issued. Hoever, this handling could be further improved through just flagging the message as spam, and if checked by the admins of the groupchat and confirmed to be spam, then by a single command we can have the Bot ban the user and therefore get rid of the spam messages.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In conclusion, the journey from acquiring the data from various sources in order to increase model’s overall performance, to deploying a working ML model implemented in a Telegram Bot to a groupchat, resulted in meeting the expectations that were set out from the start for this project. Despite the problems associated with the low data used due to the heavily class imbalance, and the possible improper classification due to the unrepresentable data, the project is still of viable and use, and with more work and future development on the data and handling of spam by the Bot, the project could easily step up its performance to go beyoned and exceed epxectations of the project.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>