---
title: "Capstone Report on Spam Detection Model Implemented on a Telegram Bot"
author: "Ahmed Almohammed"
date: "08/25/2022"
format: html
theme: sandstone
css: styles.css
code-fold: true
toc: true
jupyter: python3
---

```{python}
#| echo: false
#| output: false
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datasets import load_dataset
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import TextVectorization, Embedding, Flatten, Dense, Dropout, LSTM, GRU, Conv1D, Input, GlobalAvgPool1D, GlobalMaxPool1D, Bidirectional, Reshape

sms_data1 = pd.read_csv("../../data/SPAM text message 20170820 - Data.csv")
sms_data2 = pd.read_csv("../../data/spam.csv", encoding='latin-1')
sms_data3 = pd.DataFrame((load_dataset("sms_spam")['train']))
sms_data4 = pd.read_csv("../../data/sms_spam.csv")

# unify the features names
sms_data1.rename({"Category": "label", "Message": "message"}, axis=1, inplace=True)
sms_data1 = sms_data1[["message", "label"]]

# unify the features names & drop unwanted features
sms_data2.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1, inplace=True)
sms_data2.rename({"v1" : "label", "v2" : "message"}, axis=1, inplace=True)
sms_data2 = sms_data2[["message", "label"]]

# unify the features names
sms_data3.rename({"sms" : "message"}, axis=1, inplace=True)
sms_data3['label'] = sms_data3['label'].apply(lambda x: "spam" if x == 1 else "ham")

# unify the features names
sms_data4.rename({"type" : "label", "text" : "message"}, axis=1, inplace=True)
sms_data4 = sms_data4[["message", "label"]]

sms_big = pd.concat([sms_data1, sms_data2, sms_data3, sms_data4], axis=0)
sms_big.drop_duplicates(inplace=True)
# dropping the outliers in ham label
## getting the index of each outlier
message_lengths = []
for row in range(0, len(sms_big)):
    message_lengths.append(len(sms_big.message.iloc[row]))
sms_big['length'] = message_lengths
indices = sms_big[sms_big.length > 700].index.values
sms_big.drop(indices, axis=0, inplace=True)
```



This report illustrates the fundamental phases of the project, from data preprocessing to deployment using `pyrogram` library to build a working Telegram Bot capable of using the ML model generated to monitor the messages within a groupchat, and flag any messages that are considered potential spam.

## Introduction

This capstone project addresses the problem of spam messages being sent out via various users to public groupchats. A possible solution has been built in this project, in which a deep learning model was trained on sms messages, labelled as either spam or not, and then used in a Telegram bot to detect incoming spam messages in the deployed Telegram groupchats.

## Data Preprocessing

The data for this project was gathered from 4 various sources:

- [uciml](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) from Kaggle
- [team-ai](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification) from Kaggle
- [DeshDSingh](https://github.com/DeshDSingh/SMS-SPAM-Detection/blob/master/sms_spam.csv) from GitHub
- [sms_spam](https://huggingface.co/datasets/sms_spam) from HuggingFace Datasets


After getting the data from their sources, they were all concatenated together, dropping the resulting duplicates, to finally be left out with a large volume of data, totalling at around `11K` examples, with the following data description:

|Feature|Description|
|:--------|:---------:|
message | the sms message in text form|
label   | the classification of the message, either `spam`, or not, `ham`|

However, the data was heavily imbalanced, especially in the original sources, where the data with label `ham` were by far the dominating example in the dataset. See @fig-class.

```{python}
#| label: fig-class
#| fig-cap: "The imbalance between the 2 classes"

# plot the distribution of the examples in each class
plt.figure(figsize=(9,6))
sns.countplot(x='label', data=sms_big)

plt.title('Distribution of the Messages')
plt.xlabel('Labels')
plt.ylabel('Count')
plt.show();
```


To solve this imbalance problem, 2 methods were tried (after performing train-test split):

### Oversampling the Minority Class

The idea was to use `SMOTEN` method from `imblearn` package to successfully oversample the `ham` class. Upon creating the `SMOTEN` instance, and applying it to our data sets, the classes became balanced. However, the minority class was almost three times filled with duplicates, which is extremely unwanted in our data, especially when we proceed to model development, as the model will no doubt overfit the data and will not generalize well on unseen data.

As a result, the next method was tried.

### Undersampling the Majority Class

In performing undersampling for the majority class, we used the `RandomUnderSampler` method from `imblearn`, and then applied it on the data sets. Even though many examples from the `ham` class were truncated and left off to perform undersampling, this is still better in this case than having a `spam` class consisting of almost all duplicates.

As a result, the total number of examples left went down from about `11K` to about `3K`. @fig-train and @fig-test depict the distribution of the data in train and test sets respectively.

```{python}
#| echo: false
#| output: false

unders = RandomUnderSampler(random_state=42,replacement=True)
X = sms_big.message
y = sms_big.label
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

```


```{python}
#| label: fig-train
#| fig-cap: "The training data set"

# training data after undersampling
X_res, y_res = unders.fit_resample(np.array(X_train).reshape(-1,1), np.array(y_train).reshape(-1,1))
X_train_res = pd.Series(X_res.reshape(-1))
y_train_res = pd.Series(y_res.reshape(-1))
train_df = pd.concat([X_train_res, y_train_res], axis=1)
sns.countplot(x=1, data=train_df);
```


```{python}
#| label: fig-test
#| fig-cap: "The testing data set"

# testing data after undersampling
X_res, y_res = unders.fit_resample(np.array(X_test).reshape(-1,1), np.array(y_test).reshape(-1,1))
X_test_res = pd.Series(X_res.reshape(-1))
y_test_res = pd.Series(y_res.reshape(-1))
test_df = pd.concat([X_test_res, y_test_res], axis=1)
sns.countplot(x=1, data=test_df);
```

After splitting the data, and fixing the imbalance problem in the right way, we can proceed to the next phase of the preprocessing, whcih is Text Tokenization.

### Text Tokenization

Now, we must convert our categorical data, i.e the `message` and the `label`, into numerical data, as the deep learning models cannot handle text data, it needs to be converted to numbers. For the `label`, we can easily convert them to numerical data through using `LabelEncoder` frok `sklearn` package. After doing so, the result of applying the `LabelEncoder` looks like this:

```{python}
#| echo: false
#| output: false

# convert the datasets to numpy arrays
X_train_res = X_train_res.to_numpy()
y_train_res = y_train_res.to_numpy()

X_test_res = X_test_res.to_numpy()
y_test_res = y_test_res.to_numpy()
```


```{python}
print(f"Before applying LabelEncoder: {y_train_res[:5]}")
# apply label encoder to transform the caategorical target variable
le = LabelEncoder()
y_train_res = le.fit_transform(y_train_res)
y_test_res = le.fit_transform(y_test_res)
print(f"After applying LabelEncoder: {y_train_res[:5]}")
```

Next, we will need to first apply a Text Vectorizer on our `message` feature, and then initialize an embedding layer to be used during our experimenting with model building. The Text Vectorizer applied in this case assigns a unique number to each word (word-level tokenization) in the `message` feature, i.e the text corpus, where each word in this case is considered a token. Both the text vectorizer, along with the embedding, are used from the `tensorflow` deep learning package. After applying the text vectorization on the data, and initiating an embedded layer to be used later on, this is the result attained:

```{python}
#| echo: false
#| output: false

# create a text vectorization layer
# notice: the hyperparameters in this case have been hardcoded. You can see the original variables in the raw file.
text_vectorizer = TextVectorization(max_tokens=10000, 
                                    standardize="lower_and_strip_punctuation", 
                                    split="whitespace", 
                                    ngrams=None, 
                                    output_mode="int",
                                    output_sequence_length=19)

# apply it to our training set
text_vectorizer.adapt(X_train_res)

# initialize messages embeddings
embedding = Embedding(input_dim=len(text_vectorizer.get_vocabulary()), 
                             output_dim=128, 
                             embeddings_initializer="uniform", 
                             input_length=19) 
```

```{python}
print(f"Original message:\n{X_train_res[0]}\n\nVectorized version:\n{text_vectorizer([X_train_res[0]])}\n\nEmbedded version:\n{embedding(text_vectorizer([X_train_res[0]]))}")
```

Finally, we can now proceed to model development, as we have completed all the preprocessing necessary on our data.

## Model Development

In this phase, several deep learning model architectures were tested out on the training and test sets, including a baseline model built using a shallow learning algorithm. Because of the low number of training examples due to the heavily imbalanced problem and the undersampling made, having a high accuracy did not necessarily indicate that the model will perform well on custom data, i.e it wont necessary generalize well to unseen messages. As a result, I have created a second metric consisting of sample custom data to also use in finding the optimal model in this scenario.

Below are the architectures used for each model, and a brief overview of why it was tested out:

### Model 0: Baseline Model

```{python}
model_0 = Pipeline([
    ('tfid', TfidfVectorizer()),
    ('mnb', MultinomialNB())
])
model_0
```

This model acts as the starting ground for model development, as I try to better develop a deep neural network architecture to beat it. It uses `TfidVectorizer` (term frequency-inverse document frequency) method to map the words in our messages to unique numbers. Then it passes the output to `MultinomialNB`, which is scikit-learn go to for classification problems regarding text data.

### Model 1: Simple Deep Model

```{python}
# build the model
inputs = Input(shape=(1,), dtype='string')
x = text_vectorizer(inputs)
x = embedding(x)
x = GlobalAvgPool1D()(x)
outputs = Dense(units=1, activation='sigmoid')(x)

model_1 = Model(inputs, outputs, name='simple_model_1')

# compile the model
model_1.compile(
    loss='binary_crossentropy',
    optimizer= Adam(),
    metrics=['accuracy']
)

# get a summary of the model
model_1.summary()
```

This model starts off easy with only the usual text vectorization and embedding layers, and then a `GlobalAvgPool1D` layer to reduce the size of the representation and detect more robust features.

### Model 2: Deep Model with LSTM

```{python}

# create model_2 embedding layer
embedding = Embedding(input_dim=len(text_vectorizer.get_vocabulary()), 
                             output_dim=128, 
                             embeddings_initializer="uniform", 
                             input_length=19) 
# build the model
inputs = Input(shape=(1,), dtype='string')
x = text_vectorizer(inputs)
x = embedding(x)
x = LSTM(128)(x)
outputs = Dense(units=1, activation='sigmoid')(x)

model_2 = Model(inputs, outputs, name='lstm_model_2')

# compile the model
model_2.compile(
    loss='binary_crossentropy',
    optimizer=Adam(),
    metrics=['accuracy']
)

# summary of the model
model_2.summary()
```

We must redefine a new embedding layer to each model, as the embedding layer is considered a learned representation, where each model might have different representation for it based on its training. In this model, we used the first type of RNN, and LSTM layer (Long-Shot-Term-Memory). Its a sophisticated RNN that uses 3 main components in its computations, namely an input gate, update gate and forget gate. Together, these tend to produce accurate results for long sequences (as in our case)

### Model 3: Deep Model with GRU

```{python}

# create model_3 embedding layer
embedding = Embedding(input_dim=len(text_vectorizer.get_vocabulary()), 
                             output_dim=128, 
                             embeddings_initializer="uniform", 
                             input_length=19) 
# build the model
inputs = Input(shape=(1,), dtype='string')
x = text_vectorizer(inputs)
x = embedding(x)
x = GRU(128)(x)
outputs = Dense(units=1, activation='sigmoid')(x)

model_3 = Model(inputs, outputs, name='gru_model_3')

# compile the model
model_3.compile(
    loss='binary_crossentropy',
    optimizer=Adam(),
    metrics=['accuracy']
)

# summary of model_3
model_3.summary()
```

We also tried the second type of an RNN, namely GRU (Gated Recurrent Network). This type is not as computationaly costly as the LSTM, as it tends to have lower parameters due to its less component architecture under the hood. Even though it uses less memory and is faster than LSTM, it might not perform so well in this case.

### Model 4: Deep Model with BRNN

```{python}
# create model_4 embedding layer
embedding = Embedding(input_dim=len(text_vectorizer.get_vocabulary()), 
                             output_dim=128, 
                             embeddings_initializer="uniform", 
                             input_length=19) 
# build the model
inputs = Input(shape=(1,), dtype='string')
x = text_vectorizer(inputs)
x = embedding(x)
x = Bidirectional(LSTM(128))(x)
outputs = Dense(units=1, activation='sigmoid')(x)

model_4 = Model(inputs, outputs, name='brnn_model_4')

# compile the model
model_4.compile(
    loss='binary_crossentropy',
    optimizer=Adam(),
    metrics=['accuracy']
)
# summary of model
model_4.summary()
```

Instead of only making the training process unidirectional, we tested out on making it bidirectional, to make the alogirthm or the the network detect more features out of the data. Its use was especially intended as it can learn from the context of the message, as we think it might affect the classification of the message.

### Model 5: Deep Model with Conv1D

```{python}

# create model_5 embedding layer
embedding = Embedding(input_dim=len(text_vectorizer.get_vocabulary()), 
                             output_dim=128, 
                             embeddings_initializer="uniform", 
                             input_length=19) 
# build the model
inputs = Input(shape=(1,), dtype='string')
x = text_vectorizer(inputs)
x = embedding(x)
x = Conv1D(filters=32, kernel_size=5, activation='relu')(x)
x = GlobalMaxPool1D()(x)
x = Dense(units=32, activation='relu')(x)
outputs = Dense(units=1, activation='sigmoid')(x)

model_5 = Model(inputs, outputs, name='conv1d_model_5')

# compile the model
model_5.compile(
    loss='binary_crossentropy',
    optimizer=Adam(),
    metrics=['accuracy']
)
# summary of model
model_5.summary()
```

The use of `Conv1D` layer is of a great benefit to the training of the model, as this layer allows our network to capture the spatial data from our 1 dimensional sequences of messages, in which other RNN layers were unable to do so. Through this layer, we can utilize the power of convolutional layers used in effective computer vision problems.

### Model 6: Deep Model with Modifications

```{python}

# create model_6 embedding layer
embedding = Embedding(input_dim=len(text_vectorizer.get_vocabulary()), 
                             output_dim=128, 
                             embeddings_initializer="uniform", 
                             input_length=19) 
# build the model
inputs = Input(shape=(1,), dtype='string')
x = text_vectorizer(inputs)
x = embedding(x)
x = Bidirectional(LSTM(256))(x)
x = Reshape((512,1))(x)
x = Conv1D(filters=32, kernel_size=5, activation='relu')(x)
x = GlobalMaxPool1D()(x)
x = Dense(units=32, activation='relu')(x)
x = Dropout(0.2)(x)
x = Dense(units=16, activation='relu')(x)
outputs = Dense(units=1, activation='sigmoid')(x)

model_6 = Model(inputs, outputs, name='mods_model_6')

# compile the model
model_6.compile(
    loss='binary_crossentropy',
    optimizer=Adam(),
    metrics=['accuracy']
)
# summary of model
model_6.summary()
```

In this final model, we implemented a modified deep neural network architecture based on our experimenting with the above models and their components. After the ususal text vectorization and embedding layers, the output gets passed to a bidirectional `LSTM` layer to accurately detect features from the sequence of messages in both directions and based on the context of them in general. It then gets reshaped to fit into a `Conv1D` layer to further detect more features on top of the `LSTM` ones, through the power of convolotions. It then outputs it to a combination of max pooling and fully connected layers to reduce representation size and also a `Dropout` layer for regularization.

### Model Conclusion

Based on the above models, each model was trained on the same metrics and hyperparameters to ensure fairness for all. But like we mentioned above, becuase of the low data, the accuracy and f1-score metrics were not really helpful. Several models tend to attains almost similar results, while others unfortunately overfitted the data. With the custom data used as an additional metric, the results were as follows:

<table class="table table-hover">
  <thead>
    <tr>
      <th scope="col">Model</th>
      <th scope="col">Accuracy</th>
      <th scope="col">F1-Score</th>
      <th scope="col">Custom Data</th>
    </tr>
  </thead>
  <tbody>
    <tr class="table-info">
      <th scope="row">Model 0</th>
      <td>0.9599</td>
      <td>0.9599</td>
      <td>0.7500</td>
    </tr>
    <tr class="table-info">
      <th scope="row">Model 1</th>
      <td>0.9599</td>
      <td>0.9598</td>
      <td>0.5000</td>
    </tr>
    <tr class="table-info">
      <th scope="row">Model 2</th>
      <td>0.9775</td>
      <td>0.9775</td>
      <td>0.7500</td>
    </tr>
    <tr class="table-info">
      <th scope="row">Model 3</th>
      <td>0.9855</td>
      <td>0.9855</td>
      <td>0.7500</td>
    </tr>
    <tr class="table-info">
      <th scope="row">Model 4</th>
      <td>0.9727</td>
      <td>0.9727</td>
      <td>0.7500</td>
    </tr>
    <tr class="table-info">
      <th scope="row">Model 5</th>
      <td>0.9839</td>
      <td>0.9839</td>
      <td>0.5000</td>
    </tr>
    <tr class="table-success">
      <th scope="row">Model 6</th>
      <td>0.9743</td>
      <td>0.9743</td>
      <td>1.000</td>
    </tr>
  </tbody>
</table>

Based on the above results, we concluded that `model_6` is the optimal model for this case, as it can generalize well to unseen data, both to the custom data here and other ones used in the raw notebook.

Now, the model is ready to be deployed.

## Model Deployment

After the model development and experimenting with the various layers, we finally have a good model to use in our intended case, and that is deploy it to a Telegram groupchat and see it working in action. The following points illustrate the main steps taken into deploying a fully working Telegram bot capable of utilizing our model's predictions in detecting spam:

1. Save the model

The model was saved through the use of `TensorFlow`'s `save_model` method, which saves all the needed data for the model to work successfully when loaded in a separate folder.

2. Initializing a Telegram Bot with the help of `Pyrogram`

For this part, the Telegram bot was created with the help of several functions from the `pyrogram` package, which included

- Getting the api configuration keys
- Creating a bot account from the `BotFather` in Telegram
- Setting a user session along with a Bot session
- Adding handlers for required functionalities for the bot

3. Load the model and register it to a handler

The saved model can be easily loaded up and get it ready for prediction through `TensorFlow`'s `load_model` method. After loading the model with its weights and data, a handler function is created to monitor the messages sent in the groupchat in the following way:

- If the message is detected as potential spam, then the user gets a warning and the message is flagged as potential spam. Once reaching a 3rd warning, the last potential spam message will be deleted, and the user will be banned.

- If the message is detected as `ham` (not spam), then the bot just ignores it.

For more info on the deployment implementation, see the `app` directory.

## Problems Associated with Current Solution

As we can see from the performance of our model in classifying incoming messages in these Telegram chats, we notice that sometimes there are case were the model misclassifies a non-spam message for a spam message. This is due to several factors that have limited our model's performance in such way, mainly include:

- **Unrepresentable Data**: This is very probelmatic for our implementation, as the data that the model was trained on turned out to be not representable of the whole diversity and strucutre of the messages in sent by users in Telegram.
- **Not Enough Data**: As we mentioned in data preprocessing phase, because we had this problem of class imbalance and we had the need to undersample our data to level it with out minority class, many examples were left out and were left with only very few training data compared to how we started with. As a result, our model could not successfully learn the distinguishable points between spam and not spam messages.
- **Nature of Spam Messages**: As we can see in our modern day social media activity, we can all agree that the spam messages circulated between users and groupchats are not of consistent structure, and they tend to learn and improve their delivery of spam messages to further improve their chance of manipulating the user. It is quite hard to get hands on such data from various sources to ensure variability in the spam messages, and even harder in making our model keep up with the new types of spam messages.

## Future Development

Despite the ups and downs of this project, there have been many new ideas and thoughts in mind throughout the development phase (raises some ethical concerns):

- **Scraping Messages for new Training Data in Telegram Groupchats**: One of the main problems in this project was that the data acquired did not really match the structure of the messages sent in Telegram chats. As a result, to fix this for future needs, having a data purely from Telegram groupchats will definitely improve the model performance. However, before scraping the messages from any groupchats, the members should be notified of such action, and their permission must be given to commence such operation.
- **Storing Successfully Predicted Messages in a Data Warehouse**: Instead of having to go through the ethical concerns with scraping data from Telegram groupchats, we can make the deployed Bot in a groupchat store the successful predicted spam messages in some place like a data warehouse (csv file, on the cloud), where we can later use this set of data to better optimize our model's performance.
- **Improving the Handling of Spam by the Bot**: The current handling is done through giving warnings to potential spam senders, and banning them once a 3rd warning is issued. Hoever, this handling could be further improved through just flagging the message as spam, and if checked by the admins of the groupchat and confirmed to be spam, then by a single command we can have the Bot ban the user and therefore get rid of the spam messages.

## Conclusion

In conclusion, the journey from acquiring the data from various sources in order to increase model's overall performance, to deploying a working ML model implemented in a Telegram Bot to a groupchat, resulted in meeting the expectations that were set out from the start for this project. Despite the problems associated with the low data used due to the heavily class imbalance, and the possible improper classification due to the unrepresentable data, the project is still of viable and use, and with more work and future development on the data and handling of spam by the Bot, the project could easily step up its performance to go beyoned and exceed epxectations of the project.