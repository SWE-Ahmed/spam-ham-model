{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages Monitoring Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dealing with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# for data visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for processing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for evaluation metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sms-spam-collection-dataset.zip to /Users/necro_kudo/Developer/misk-dsi/capstone-project\n",
      "100%|█████████████████████████████████████████| 211k/211k [00:00<00:00, 680kB/s]\n",
      "100%|█████████████████████████████████████████| 211k/211k [00:00<00:00, 678kB/s]\n",
      "Downloading spam-text-message-classification.zip to /Users/necro_kudo/Developer/misk-dsi/capstone-project\n",
      "100%|█████████████████████████████████████████| 208k/208k [00:00<00:00, 472kB/s]\n",
      "100%|█████████████████████████████████████████| 208k/208k [00:00<00:00, 471kB/s]\n",
      "Archive:  sms-spam-collection-dataset.zip\n",
      "  inflating: spam.csv                \n",
      "Archive:  spam-text-message-classification.zip\n",
      "  inflating: SPAM text message 20170820 - Data.csv  \n"
     ]
    }
   ],
   "source": [
    "# download the kaggle datasets\n",
    "# uncomment the following lines to download the datasets\n",
    "# !kaggle datasets download -d uciml/sms-spam-collection-dataset\n",
    "# !kaggle datasets download -d team-ai/spam-text-message-classification\n",
    "# !wget https://raw.githubusercontent.com/DeshDSingh/SMS-SPAM-Detection/master/sms_spam.csv\n",
    "# !unzip sms-spam-collection-dataset.zip\n",
    "# !unzip spam-text-message-classification.zip\n",
    "# !rm sms-spam-collection-dataset.zip\n",
    "# !rm spam-text-message-classification.zip\n",
    "# !mv sms_spam.csv ./../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sms_spam (/Users/necro_kudo/.cache/huggingface/datasets/sms_spam/plain_text/1.0.0/53f051d3b5f62d99d61792c91acefe4f1577ad3e4c216fb0ad39e30b9f20019c)\n",
      "100%|██████████| 1/1 [00:00<00:00, 304.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# read in the datasets\n",
    "sms_data1 = pd.read_csv(\"../data/SPAM text message 20170820 - Data.csv\")\n",
    "sms_data2 = pd.read_csv(\"../data/spam.csv\", encoding='latin-1')\n",
    "sms_data3 = pd.DataFrame((load_dataset(\"sms_spam\")['train']))\n",
    "sms_data4 = pd.read_csv(\"./../data/sms_spam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the features names\n",
    "sms_data1.rename({\"Category\": \"label\", \"Message\": \"message\"}, axis=1, inplace=True)\n",
    "sms_data1 = sms_data1[[\"message\", \"label\"]]\n",
    "\n",
    "# unify the features names & drop unwanted features\n",
    "sms_data2.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\n",
    "sms_data2.rename({\"v1\" : \"label\", \"v2\" : \"message\"}, axis=1, inplace=True)\n",
    "sms_data2 = sms_data2[[\"message\", \"label\"]]\n",
    "\n",
    "# unify the features names\n",
    "sms_data3.rename({\"sms\" : \"message\"}, axis=1, inplace=True)\n",
    "sms_data3['label'] = sms_data3['label'].apply(lambda x: \"spam\" if x == 1 else \"ham\")\n",
    "\n",
    "# unify the features names\n",
    "sms_data4.rename({\"type\" : \"label\", \"text\" : \"message\"}, axis=1, inplace=True)\n",
    "sms_data4 = sms_data4[[\"message\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22277 entries, 0 to 5558\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   message  22277 non-null  object\n",
      " 1   label    22277 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 522.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# concatenate the results to form a bigger dataframe of all the previous ones\n",
    "sms_big = pd.concat([sms_data1, sms_data2, sms_data3, sms_data4], axis=0)\n",
    "sms_big.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Since we concatenated the dataframes together, we need to check for duplicates if present, and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations before dropping duplicates: 22277\n",
      "Number of duplicated observations: 10579\n",
      "Number of observations after dropping the duplicates: 11698\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates\n",
    "print(f\"Number of observations before dropping duplicates: {len(sms_big)}\")\n",
    "print(f\"Number of duplicated observations: {sms_big.duplicated().sum()}\")\n",
    "sms_big.drop_duplicates(inplace=True)\n",
    "print(f\"Number of observations after dropping the duplicates: {len(sms_big)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the number of duplicates is alarming (almost 1/2 of the concatenated data), luckily, dropping them will leave us with still a large dataset of over 10K observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     10124\n",
       "spam     1574\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the number of observations for each class\n",
    "sms_big.label.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
